{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import necessary packages\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import shelve\n",
    "import scipy\n",
    "from sklearn.externals import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling on Ohio's Restaurant Yelp Review Data: Comparison Between Latent Dirichelet Allocation and Multinomial Logistic Regression\n",
    "\n",
    "**Author:** Ningning Long, Yue You, Tian Xia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When running business like restaurants or cafes, owners of business care a lot of how they can make uses of customers’ \n",
    "reviews after they visit their businesses. The insights mined from customers’ reviews will help discover the weaknesses of the business and contribute to the improvements of service, product or ambience. This study is aimed for utilizing machine learning and natural language processing (NLP) techniques to analyze customers’ reviews from the [Yelp Dataset Challenge](https://www.yelp.com/dataset/challenge)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We focus on a subset data of the whole dataset challenge (i.e. the restaurants reviews in the state of Ohio). We are interested in using the latent dirichelet allocation (LDA) method to find out the topics underlying the customer’s reviews. The latent dirichelet allocation (LDA) is an unsupervised machine learning algorithm that can generate topics based on word frequency from a set of documents. We will use the topics to investigate how they match the reviews and predict the customers’ ratings. Meanwhile, in order to make a comparison, we will also compute the Tf-idf of reviews and use them to run the multinomial logistic regression of customers’ ratings. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abstract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Customers always provide feedback to a business, a product, or a service on websites like Yelp, while potential customers would refer to those reviews to make a decision of whether to go to a business or not. For any business on Yelp, we can see all the reviews customers provided, the overall ratings (from one star to five stars) it received, the average price level, etc. Sometimes, it is too time-consuming to go through each review about a business, so people only pay attention to the ratings. However, an overall rating cannot convey the information that led a reviewer to that experience, and people have different standards of rating, so it may be misleading. For example, if a person cares a lot about the restaurant's ambiance and goes to a restaurant with below-average ambiance but high rating because of its the taste of its food, this person may feel disappointed. Therefore, we hope to know extract the topics involved in the customers' reviews, in order to see the influential factors of the customer ratings. \n",
    "\n",
    "Therefore, in this project, we aim at identifying topics involved in every review. Also, in order to determine the association between the reviews and the customer ratings, we apply two methods: the traditional linear regression with the topics probability generated by LDA and the multinomial logistics regression with the Tf-idf word frequency. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Clean and Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The raw data are JSON files from the Round 10 [Yelp Dataset Challenge](https://www.yelp.com/dataset/challenge). We subset and extract part of the data and save it in the **'data'** folder. The *'restaurant.csv'* file is the meta-data for the restaurants in the state of Ohio. The *'reviews.csv'* file is the file that we mainly worked with, which contains the customers' reviews for restaurants in the state of Ohio. Our latent dirichelet allocation (LDA) model and multinomial logistic regression model mainly use the reviews in that file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a quick look of the *'restaurant.csv'* file in the **'data'** folder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>state</th>\n",
       "      <th>city</th>\n",
       "      <th>address</th>\n",
       "      <th>name</th>\n",
       "      <th>business_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>review_count</th>\n",
       "      <th>categories</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>OH</td>\n",
       "      <td>Painesville</td>\n",
       "      <td>1 S State St</td>\n",
       "      <td>Sidewalk Cafe Painesville</td>\n",
       "      <td>Bl7Y-ATTzXytQnCceg5k6w</td>\n",
       "      <td>3.0</td>\n",
       "      <td>26</td>\n",
       "      <td>['American (Traditional)', 'Breakfast &amp; Brunch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>OH</td>\n",
       "      <td>Northfield</td>\n",
       "      <td>10430 Northfield Rd</td>\n",
       "      <td>Zeppe's Pizzeria</td>\n",
       "      <td>7HFRdxVttyY9GiMpywhhYw</td>\n",
       "      <td>3.0</td>\n",
       "      <td>7</td>\n",
       "      <td>['Pizza', 'Caterers', 'Italian', 'Wraps', 'Eve...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>OH</td>\n",
       "      <td>Mentor</td>\n",
       "      <td>9209 Mentor Ave</td>\n",
       "      <td>Firehouse Subs</td>\n",
       "      <td>lXcxSdPa2m__LqhsaL9t9A</td>\n",
       "      <td>3.5</td>\n",
       "      <td>9</td>\n",
       "      <td>['Restaurants', 'Sandwiches', 'Delis', 'Fast F...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>OH</td>\n",
       "      <td>Cleveland</td>\n",
       "      <td>13181 Cedar Rd</td>\n",
       "      <td>Richie Chan's Chinese Restaurant</td>\n",
       "      <td>Pawavw9U8rjxWVPU-RB7LA</td>\n",
       "      <td>3.5</td>\n",
       "      <td>22</td>\n",
       "      <td>['Chinese', 'Restaurants']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>OH</td>\n",
       "      <td>Northfield</td>\n",
       "      <td>134 E Aurora Rd</td>\n",
       "      <td>Romeo's Pizza</td>\n",
       "      <td>RzVHK8Jfcy8RvXjn_z3OBw</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4</td>\n",
       "      <td>['Restaurants', 'Pizza']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  state         city              address                              name  \\\n",
       "0    OH  Painesville         1 S State St         Sidewalk Cafe Painesville   \n",
       "1    OH   Northfield  10430 Northfield Rd                  Zeppe's Pizzeria   \n",
       "2    OH       Mentor      9209 Mentor Ave                    Firehouse Subs   \n",
       "3    OH    Cleveland       13181 Cedar Rd  Richie Chan's Chinese Restaurant   \n",
       "4    OH   Northfield      134 E Aurora Rd                     Romeo's Pizza   \n",
       "\n",
       "              business_id  stars  review_count  \\\n",
       "0  Bl7Y-ATTzXytQnCceg5k6w    3.0            26   \n",
       "1  7HFRdxVttyY9GiMpywhhYw    3.0             7   \n",
       "2  lXcxSdPa2m__LqhsaL9t9A    3.5             9   \n",
       "3  Pawavw9U8rjxWVPU-RB7LA    3.5            22   \n",
       "4  RzVHK8Jfcy8RvXjn_z3OBw    4.0             4   \n",
       "\n",
       "                                          categories  \n",
       "0  ['American (Traditional)', 'Breakfast & Brunch...  \n",
       "1  ['Pizza', 'Caterers', 'Italian', 'Wraps', 'Eve...  \n",
       "2  ['Restaurants', 'Sandwiches', 'Delis', 'Fast F...  \n",
       "3                         ['Chinese', 'Restaurants']  \n",
       "4                           ['Restaurants', 'Pizza']  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv('./data/restaurant.csv',index_col=0).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a quick look of the *'reviews.csv'* file in the **'data'** folder. We will primarily work on the 'text' column, which stores the customers' reviews, and the 'stars' column, which stores the actual customers' ratings to restaurants, in our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id</th>\n",
       "      <th>cool</th>\n",
       "      <th>date</th>\n",
       "      <th>funny</th>\n",
       "      <th>review_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>text</th>\n",
       "      <th>useful</th>\n",
       "      <th>user_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>tulUhFYMvBkYHsjmn30A9w</td>\n",
       "      <td>1</td>\n",
       "      <td>2013-11-19</td>\n",
       "      <td>0</td>\n",
       "      <td>FsS5TUFPI8QJEE60-HR3dw</td>\n",
       "      <td>2</td>\n",
       "      <td>Wished it was better..\\nAfter watching man vs....</td>\n",
       "      <td>1</td>\n",
       "      <td>bWh4k_cCuVt5GLVd33xIxg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>tulUhFYMvBkYHsjmn30A9w</td>\n",
       "      <td>1</td>\n",
       "      <td>2014-12-18</td>\n",
       "      <td>0</td>\n",
       "      <td>7xGHiLP1vAaGmX6srC_XXw</td>\n",
       "      <td>4</td>\n",
       "      <td>Decor and service leave much to be desired, bu...</td>\n",
       "      <td>0</td>\n",
       "      <td>nQ4e81UdfczimYcIUtO3HA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>tulUhFYMvBkYHsjmn30A9w</td>\n",
       "      <td>1</td>\n",
       "      <td>2014-09-12</td>\n",
       "      <td>0</td>\n",
       "      <td>ZWlXWc9LHPLiOksrp-enyw</td>\n",
       "      <td>5</td>\n",
       "      <td>My husband and I ate here tonight for the firs...</td>\n",
       "      <td>0</td>\n",
       "      <td>gJPa95ZRozMhiOqvENpspA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>tulUhFYMvBkYHsjmn30A9w</td>\n",
       "      <td>1</td>\n",
       "      <td>2012-02-28</td>\n",
       "      <td>1</td>\n",
       "      <td>KpRwKYyQ93ypyDSdA7IXfw</td>\n",
       "      <td>2</td>\n",
       "      <td>Don't believe the hype. Nooooo! \\n\\nIn the Cle...</td>\n",
       "      <td>5</td>\n",
       "      <td>bAwfPH4lXNzgcYp9JFy6ow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>tulUhFYMvBkYHsjmn30A9w</td>\n",
       "      <td>3</td>\n",
       "      <td>2014-10-06</td>\n",
       "      <td>6</td>\n",
       "      <td>OZvrgp4vWBsYqIt3-YMSEw</td>\n",
       "      <td>3</td>\n",
       "      <td>Don't believe the hype!\\n\\nAfter seeing this l...</td>\n",
       "      <td>10</td>\n",
       "      <td>BjtJ3VkMOxV2Lan037AFuw</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               business_id  cool        date  funny               review_id  \\\n",
       "52  tulUhFYMvBkYHsjmn30A9w     1  2013-11-19      0  FsS5TUFPI8QJEE60-HR3dw   \n",
       "53  tulUhFYMvBkYHsjmn30A9w     1  2014-12-18      0  7xGHiLP1vAaGmX6srC_XXw   \n",
       "54  tulUhFYMvBkYHsjmn30A9w     1  2014-09-12      0  ZWlXWc9LHPLiOksrp-enyw   \n",
       "55  tulUhFYMvBkYHsjmn30A9w     1  2012-02-28      1  KpRwKYyQ93ypyDSdA7IXfw   \n",
       "56  tulUhFYMvBkYHsjmn30A9w     3  2014-10-06      6  OZvrgp4vWBsYqIt3-YMSEw   \n",
       "\n",
       "    stars                                               text  useful  \\\n",
       "52      2  Wished it was better..\\nAfter watching man vs....       1   \n",
       "53      4  Decor and service leave much to be desired, bu...       0   \n",
       "54      5  My husband and I ate here tonight for the firs...       0   \n",
       "55      2  Don't believe the hype. Nooooo! \\n\\nIn the Cle...       5   \n",
       "56      3  Don't believe the hype!\\n\\nAfter seeing this l...      10   \n",
       "\n",
       "                   user_id  \n",
       "52  bWh4k_cCuVt5GLVd33xIxg  \n",
       "53  nQ4e81UdfczimYcIUtO3HA  \n",
       "54  gJPa95ZRozMhiOqvENpspA  \n",
       "55  bAwfPH4lXNzgcYp9JFy6ow  \n",
       "56  BjtJ3VkMOxV2Lan037AFuw  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv('./data/reviews.csv',index_col=0).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In short summary of the data, we focus on the 316 restaurants which have 100 customers' reviews at least, in the state of Ohio. The maximum number of reviews for a single restaurant in our sample is around 900. The distribution of mean star ratings received for restaurants is skewed with the peak around 4 stars."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./fig/NumberOfReviewsPerRestaurant.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./fig/MeanRatings.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topics Analysis with Latent Dirichelet Allocation (LDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Manipulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before applying the LDA model, we manipulate the customer reviews in order to get the tokenized review data. For instance, an original review from the Yelp dataset is shown as the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wished it was better..\n",
      "After watching man vs. food I decided to stop by, décor was not that homey and welcoming, and the neighborhood was bad, but nothing I haven't been around before.  The ribs were very fatty and grisly, it was disappointing and I didn't get enough sauce and when I asked for a little more they wanted to charge me, the coleslaw was awesome!  I noticed a hair in my food and it turned me off to the rest of it, so i threw it away , I wont be returning...\n",
      "sorry guys\n"
     ]
    }
   ],
   "source": [
    "with shelve.open('result/first_review_train') as result:\n",
    "    first_review_train = result['first_review_train'] \n",
    "\n",
    "print(first_review_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to find the topics, we need to tokenize the reviews. First, we change all the words in each review as the lower case for convenience in the topic analysis. Then, we apply some string manipulation in order to save the meaningful words and numbers. Next, we delete all the stop words from each review, which are certain parts of English speech, like (for, or) or the words that are meaningless to the topic model. Finally, we decide to only keep the words that are the noun for further analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the text manipulation, the above raw review becomes the following word list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['man', 'vs', 'food', 'cor', 'homey', 'neighborhood', 'nothing', 'ribs', 'sauce', 'charge', 'coleslaw', 'awesome', 'hair', 'food', 'rest', 'threw']\n"
     ]
    }
   ],
   "source": [
    "with shelve.open('result/text_array') as result:\n",
    "    text_array = result['text_array']\n",
    "\n",
    "print(text_array[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The word list contains the words that talking about the food, service, and location of the restaurants, which are helpful for our topic modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we use the Dictionary() function to traverse the **text_array**, assigning a unique integer id to each unique token while also collecting word counts and relevant statistics. Next, we use doc2bow() function converts dictionary into a bag-of-words. The details can be found in **LDA.ipynb**. The corpus after these processes is used as input in LDA model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Set and Validation Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to fit and apply LDA model, we split data set into a training set and a validation set. We decide to set the validation set as the 896 reviews from the restaurant that have the largest number of reviews. Then, the rest of the reviews are considered as training set. The training set is used to train the LDA model and find the topics, and the trained LDA model is applied on the validation set for further analysis and linear regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Latent Dirichelet Allocation Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To discover latent topics in each review, we use Latent Dirichlet Allocation (LDA), a topic model that generates topics based on word frequency from a set of documents. LDA assumes that (1) documents contain multiple latent topics; (2) each document is assumed to be generated by a generative process defined by probabilistic model; and (3) each topic is characterized by a distribution over a fixed vocabulary. More specically, the joint distribution of the hidden topics and observed variables (words) is:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$p(\\phi_{1:K}, \\theta_{1:D}, Z_{1:D}, W_{1:D} ) = \\prod_{i=1}^K p(\\phi_{i}) \\prod_{d=1}^D p(\\theta_{d}) \\prod_{n=1}^N p(Z_{d,n}|\\theta_{d}) p(W_{d,n}|\\phi_{1:K}, Z_{d,n})$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where\n",
    "$$\\phi_{1:K}: the\\ topics, each\\ \\phi_k\\ is\\ a\\ distribution\\ over\\ the\\ vocabulary\\ ; \\phi_{k}\\sim Dirichlet_V(\\beta)$$   \n",
    "\n",
    "$$\\theta_{1:D}: the\\ topic\\ proportion\\ for\\ document\\ 1:D;\\ \\theta_d \\sim Dirichlet_K(\\alpha)$$\n",
    "\n",
    "$$Z_{1:D}: the\\ topic\\ assignments\\ for\\ document\\ 1:D;\\ Z_d \\sim Multinomial_K(\\theta_d)$$\n",
    "\n",
    "$$W_{1:D}: the\\ observed\\ words\\ for\\ document\\ 1:D;\\ W_d \\sim Multinomial_V(\\phi_z)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LDA learns the distributions (e.g. the distribution of a set of topics, their associated word probabilities, the topic of each word, and the particular topic mixture of each document) by using Bayesian inference. After repeating the updating process for a large number of times, the model will reach a steady state and can be used to estimate the hidden topics, topic mixtures of each document and the words associated with each topic. We use the **LdaModel** in gensim package to apply the LDA model to our training set. When fitting the model, We have tried 6, 8, 10, 12, 15 as the number of topics, and it looks like that the number of topics = 10 works the best. Thus, we save the model with 10 topics for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.109*\"food\" + 0.075*\"place\" + 0.040*\"service\" + 0.024*\"time\" + 0.020*\"restaurant\" + 0.016*\"menu\" + 0.013*\"staff\" + 0.012*\"everything\"'),\n",
       " (1,\n",
       "  '0.047*\"pizza\" + 0.028*\"place\" + 0.017*\"cleveland\" + 0.011*\"time\" + 0.010*\"way\" + 0.009*\"line\" + 0.009*\"root\" + 0.009*\"home\"'),\n",
       " (2,\n",
       "  '0.052*\"food\" + 0.035*\"time\" + 0.034*\"service\" + 0.019*\"experience\" + 0.018*\"order\" + 0.018*\"night\" + 0.015*\"place\" + 0.015*\"restaurant\"'),\n",
       " (3,\n",
       "  '0.018*\"sauce\" + 0.015*\"salad\" + 0.014*\"flavor\" + 0.014*\"dinner\" + 0.013*\"pork\" + 0.013*\"chicken\" + 0.013*\"meal\" + 0.011*\"cream\"'),\n",
       " (4,\n",
       "  '0.055*\"thai\" + 0.037*\"sushi\" + 0.035*\"spicy\" + 0.032*\"rice\" + 0.030*\"roll\" + 0.025*\"tea\" + 0.024*\"curry\" + 0.020*\"shrimp\"'),\n",
       " (5,\n",
       "  '0.086*\"coffee\" + 0.069*\"breakfast\" + 0.054*\"brunch\" + 0.027*\"bacon\" + 0.020*\"egg\" + 0.019*\"toast\" + 0.018*\"morning\" + 0.018*\"hash\"'),\n",
       " (6,\n",
       "  '0.066*\"beef\" + 0.030*\"pho\" + 0.026*\"pork\" + 0.025*\"soup\" + 0.020*\"cleveland\" + 0.016*\"bowl\" + 0.014*\"pot\" + 0.014*\"meat\"'),\n",
       " (7,\n",
       "  '0.067*\"beer\" + 0.053*\"place\" + 0.047*\"bar\" + 0.047*\"food\" + 0.034*\"selection\" + 0.020*\"service\" + 0.018*\"night\" + 0.017*\"cleveland\"'),\n",
       " (8,\n",
       "  '0.043*\"sandwich\" + 0.034*\"wife\" + 0.028*\"bbq\" + 0.024*\"dog\" + 0.020*\"meat\" + 0.019*\"melt\" + 0.017*\"beef\" + 0.017*\"chicken\"'),\n",
       " (9,\n",
       "  '0.109*\"burger\" + 0.051*\"hour\" + 0.030*\"bar\" + 0.023*\"tacos\" + 0.023*\"bartender\" + 0.022*\"spot\" + 0.019*\"b\" + 0.015*\"taco\"')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read in the lda model\n",
    "ldamodel = joblib.load(\"result/finalized_model_10.sav\")\n",
    "\n",
    "# the topics found by lda model\n",
    "ldamodel.print_topics(num_topics=10, num_words=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The LDA model finds the 10 topics, and we have shown the 10 topics with 8 highest frequent words in each topic as above. The 10 topics are relatively interpretable. By associating and categorizing the high-frequency words of each topic, we name the topics as the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 'Service1')\n",
      "(1, 'Location')\n",
      "(2, 'Service2')\n",
      "(3, 'American1')\n",
      "(4, 'Asian1')\n",
      "(5, 'Breakfast')\n",
      "(6, 'Asian2')\n",
      "(7, 'Bar')\n",
      "(8, 'American2')\n",
      "(9, 'Mexican')\n"
     ]
    }
   ],
   "source": [
    "with shelve.open('result/topic_name') as result:\n",
    "    topic_dict = result['topic_name']\n",
    "\n",
    "for keys,values in topic_dict.items():\n",
    "    print((keys, values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Further Interpretation of Topics with A Review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to see if the topic modeling makes sense, we have extracted a review from the validation set and applied LDA to find the topics probability of this review."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the original review of the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If I didn't have to pay the bill, I'd enjoy this restaurant a lot more.\n",
      "\n",
      "Yea, yea, I know -- I could say that about any place. But this one seems to fit that statement more than almost any other in Cleveland. \n",
      "\n",
      "Two burgers -- perfectly cooked and well seasoned with just the right amount of salt and other mouth-watering dashes of spice (plus the bun was nicely seasoned, something that many burger joints neglect). A decent side of fries. About a dozen chicken wings -- Falling off the bone and \"Chef-ed up\" with lemon juice, scallions, jalapeno and garlic, not simply smothered in a thick reddish-orange sauce.\n",
      "\n",
      "But why does all of that still have to cost $50? (And those were some of the least expensive items on the menu). \n",
      "\n",
      "Nevertheless, the drinks were great -- I had two different unique takes on the Old-Fashioned (who would have thought that Curacao works with Bourbon) and Jeannene had a new spin on the French 75 before trying one of the Old-Fashioneds -- and they were well worth another $50.\n",
      "\n",
      "Coupled with attentive service and a great patio seating on E. 4th Street, I am happy to give it four stars. But the prices seem to promise a 5-star experience.\n"
     ]
    }
   ],
   "source": [
    "with shelve.open('result/first_review_vali') as result:\n",
    "    example_review = result['first_review_vali']\n",
    "\n",
    "print(example_review)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, let's see the topics probability associated with this review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2, 0.29715302229392604),\n",
       " (0, 0.24879956111281701),\n",
       " (9, 0.21169559380609276),\n",
       " (3, 0.16659952745901788),\n",
       " (4, 0.057228951701560421)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with shelve.open('result/first_review_topics_vali') as result:\n",
    "    topic_prob = result['first_review_topics_vali']\n",
    "\n",
    "topic_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Topics 0 and 2 are the service topics. Topic 9, 3, and 4 represent Mexican food, American food, and Asian food respectively. We can see the topics are relatively reasonable, since the review talks a lot about the service and the service topics have the highest probability in this review. However, according to the review, the restaurant seems to be an American restaurant, but Mexican food topic has the highest probability among the 3 topics talking about food. It shows that the topics modeling may not be perfect to describe every part of the restaurant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, we do not a high expectation about the prediction accuracy for customer ratings according to the topics, and the next section confirmed our concern."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Customer Rating Prediction with Topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we are trying to use the topics probability found by LDA model to predict the rating given by the customers. Traditional linear regression is applied here, in order to see if the topics from a review are highly associated with the customer rating."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we use the probability of each topics as the elements of the design matrix. After creating the design matrix, we decide to merge few topics together, since they reflect the similar contents. We merge topics 0 and 2 together, since they both reflect the service of the restaurants. Topic 3 and 8 are merged, because they both represent American food. Finally, topic 4 and 6 are merged, since they are both Asian food topics. Thus, the design matrix eventually has 7 different features. Our response variable is the customer ratings. Also, we apply MSE (Mean Squared Error) as a metric of accuracy, as the original ratings are integers and predicted ratings are float numbers. The regression results are the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients\n",
      "[-3.37275057 -2.04183867 -2.21252578 -1.61272055 -1.7564765  -1.5980306\n",
      " -3.56868465]\n",
      "Intercept\n",
      "6.27726471738\n",
      "Mean squared error\n",
      "1.6797105653141962\n"
     ]
    }
   ],
   "source": [
    "# read in the linear regression result\n",
    "with shelve.open('result/lm_result') as result:\n",
    "    lm_result =  result['lm_result']\n",
    "\n",
    "for keys,values in lm_result.items():\n",
    "    print(keys)\n",
    "    print(values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above result, we can see that the MSE is pretty high, which means using the topics probability to predict customer ratings is not very accurate for our data. Therefore, another supervised learning model---Multinomial Logistic Regression is applied to the data as well. The analysis and results can be found in another notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, why using the topics probability to predict customer ratings is not working here? There are several possible explainations. For instance, when we are training the LDA model, We have only tried 6, 8, 10, 12, 15 as the number of topics and chosen 10 as the best one because of time limitaion. There may be a better choice of number of topics that fits the data. Therefore, we believe that if we keep tunning the parameters in the LDA model, the prediction results can be better. Also, fitting the LDA model with larger dataset can be helpful as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multinomial Logestic Regression with Tf-idf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF transformation\n",
    "#### Definition of TF-IDF Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF means \"Term Frequency - Inverse Document Frequency\". It is a powerful technique to detect important words in a collection of documents. \"Term Frequency\" (TF) meansures the frequency of word $w_i$ in document $d_j$, and the \"Inverse Document Frequency\" (IDF) measures how much information the word provides, i.e., the frequency of word $w_i$ in the collection of documents. The TF-IDF value for a word $w_i$ in document $d_j$ is positively associated with word frequencies and negatively associated with document frequencies. The math formula for TF-IDF is:\n",
    "\n",
    "$$TF-IDF(w_i, d_j) = TF(w_i, d_j) \\times IDF(w_i)$$\n",
    "\n",
    "And IDF can be smoothed using the formula:\n",
    "\n",
    "$$IDF_{smooth}(w_i) = log(\\frac{N}{1 + n_i})$$\n",
    "\n",
    "where $N$ is the number of documents considered and $n_i$ is the frequency of $w_i$ in the all documents considered.\n",
    "\n",
    "In this project, TF-IDF is used in logistic regression classification. In the following analysis, we did several steps to fit the best logistic regression model:\n",
    "\n",
    "#### Steps in TF-IDF Transformation\n",
    "1. constructed the TF-IDF matrix, the matrix is 'text_features', a sparse matrix. Also, the reponses of all observations are in 'star', an array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load transformed data\n",
    "text_features = scipy.sparse.load_npz('result/text_features.npz')\n",
    "star = np.load('result/star.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of observations in the text_features dataset is 60222 \n",
      "Number of covariates in the text_features dataset is 50137\n",
      "The format of text_feature is\n",
      "   (0, 17785)\t0.060146376659\n",
      "  (0, 19534)\t0.0664307345244\n",
      "  (0, 37080)\t0.0971322128585\n",
      "  (0, 6868)\t0.215221170104\n",
      "  (0, 41758)\t0.14424586198\n",
      "  (0, 16242)\t0.115560138128\n",
      "  (0, 3668)\t0.169463104229\n",
      "  (0, 33687)\t0.18134186701\n",
      "  (0, 42347)\t0.244903534338\n",
      "  (0, 30214)\t0.173935675158\n",
      "  (0, 19663)\t0.219337002949\n",
      "  (0, 10430)\t0.210710863093\n",
      "  (0, 38753)\t0.253060975895\n",
      "  (0, 11918)\t0.548103837493\n",
      "  (0, 1483)\t0.283007805783\n",
      "  (0, 2376)\t0.371248887227\n",
      "  (0, 43674)\t0.274472118608\n",
      "The format of star is\n",
      " [2 4 5 ..., 4 5 5]\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of observations in the text_features dataset is\", text_features.shape[0],\n",
    "      \"\\nNumber of covariates in the text_features dataset is\", text_features.shape[1])\n",
    "print(\"The format of text_feature is\\n\", text_features[-1:])\n",
    "print(\"The format of star is\\n\", star)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Next Step: \n",
    "4. splited the whole dataset into training set and validation set using 10-fold cross-valudation,\n",
    "5. used the _TF-IDF values_ as covariates, the _star values_ of review as responses, to build a logistic regression model in the training set,\n",
    "6. tried 3 different tuning parameters respectively,\n",
    "7. applied the models built in training set to validation set and obtained the predicted _star values_ for each tuning parameter,\n",
    "8. computed the Mean Squared Error (MSE) between true _star value_ and predicted _star value_ in validation set, \n",
    "9. and chose the optimal tuning parameters which produces lowest MSE.\n",
    "\n",
    "#### Here, we run a function called 'compute_CV_mse' to do the rest steps above. \n",
    "**We computed MSE with 10 fold cross-validation, of first 1,000 keywords, with random splitting seed for training and validation sets = 1, and original tuning parameters = (1, 100, 1000, 10000, 100000). After  many trails, we selected the current range [10, 100] as the optimal range of tuning parameters.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we showed the output MSE with corresponding tuning parameters, sorted smallest to largest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sorted MSE and corresponding parameters: small to big\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mse</th>\n",
       "      <th>parameters</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.748335</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.749202</td>\n",
       "      <td>40.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.749553</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.749811</td>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.749885</td>\n",
       "      <td>60.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.750014</td>\n",
       "      <td>70.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.751435</td>\n",
       "      <td>80.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.753058</td>\n",
       "      <td>90.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.753077</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.753630</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        mse  parameters\n",
       "4  0.748335        50.0\n",
       "3  0.749202        40.0\n",
       "1  0.749553        20.0\n",
       "2  0.749811        30.0\n",
       "5  0.749885        60.0\n",
       "6  0.750014        70.0\n",
       "7  0.751435        80.0\n",
       "8  0.753058        90.0\n",
       "0  0.753077        10.0\n",
       "9  0.753630       100.0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sorted = pd.read_hdf('result/df_sorted.h5', 'df_sorted')\n",
    "print(\"Sorted MSE and corresponding parameters: small to big\")\n",
    "df_sorted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the trend of MSE by tuning parameter is plotted:\n",
    "<img src=\"./fig/mse_logistic.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The minimum MSE is 0.7483 with tuning parameter = 50.0\n"
     ]
    }
   ],
   "source": [
    "print(\"The minimum MSE is\", np.round(df_sorted.iloc[0][0], 4), \"with tuning parameter =\", df_sorted.iloc[0][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion:\n",
    "\n",
    "**The minimum Cross-validated MSE is 0.7483 with tuning parameter = 50**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Author Contributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "This repository and project is the collaboration from **Ningning Long**, **Yue You** and **Tian Xia**. Their contributions to the project are summarized as:\n",
    "\n",
    "**Ningning Long**:\n",
    "-\t‘LDA.ipynb’, latent dirichelet allocation modeling and analysis\n",
    "-\t‘environment.yml’\n",
    "-\tSome write-up of ‘main.ipynb’\n",
    "\n",
    "**Yue You**:\n",
    "-\t‘Logistic.ipynb’, statistical modeling of multinomial logistic regression\n",
    "-\t‘.gitignore’\n",
    "-\tSome write-up of ‘main.ipynb’\n",
    "\n",
    "**Tian Xia**:\n",
    "-\t‘Data_Cleaning.ipynb’, data extraction and cleaning\n",
    "-\t‘README.md’\n",
    "-\tSome write-up of ‘main.ipynb’\n",
    "-\t‘LICENSE.md’\n",
    "-\t‘Makefile’\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
